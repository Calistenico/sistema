name: anythingllm

services:
  anything-llm:
    container_name: anything-llm
    image: anything-llm:latest
    build:
      context: ../.
      dockerfile: ./docker/Dockerfile
      args:
        ARG_UID: ${UID:-1000}
        ARG_GID: ${GID:-1000}
    cap_add:
      - SYS_ADMIN
    volumes:
      - "./.env:/app/server/.env"
      - "../server/storage:/app/server/storage"
      - "../collector/hotdir/:/app/collector/hotdir"
      - "../collector/outputs/:/app/collector/outputs"
    user: "${UID:-1000}:${GID:-1000}"
    ports:
      - "3001:3001"
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"

  llamacpp_model:
    volumes:
      - ./.models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf:/models:ro
      - /usr/lib/wsl:/usr/lib/wsl
    # CPU/GPU support Intel (required for server-intel image)
    devices:
      - /dev/dri:/dev/dri
      - /dev/dxg:/dev/dxg
    # Expose llamacpp API outside the container stack
    ports:
      - 8086:8086
    container_name: llamacpp_model
    command: --host 0.0.0.0 --port 8086 -m /models -c 2048 --chat-template zephyr # server-intel is unstable and will generate garbage easily, trying to control for that with low context model
    image: ghcr.io/ggerganov/llama.cpp:server-intel
