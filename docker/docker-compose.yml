# version: "3.9"

name: anythingllm

networks:
  anything-llm:
    driver: bridge

services:
  anything-llm:
    container_name: anything-llm
    image: anything-llm:latest
    build:
      context: ../.
      dockerfile: ./docker/Dockerfile
      args:
        ARG_UID: ${UID:-1000}
        ARG_GID: ${GID:-1000}
    cap_add:
      - SYS_ADMIN
    volumes:
      - "./.env:/app/server/.env"
      - "../server/storage:/app/server/storage"
      - "../collector/hotdir/:/app/collector/hotdir"
      - "../collector/outputs/:/app/collector/outputs"
    user: "${UID:-1000}:${GID:-1000}"
    ports:
      - "3001:3001"
    env_file:
      - .env
    networks:
      - anything-llm
    extra_hosts:
      - "host.docker.internal:host-gateway"

  llamacpp_model:
    volumes:
      - ./.models/rocket-3b.Q4_K_M.gguf:/models:ro
    # Expose llamacpp API outside the container stack
    ports:
      - 8086:8086
    container_name: llamacpp_model
    command: --host 0.0.0.0 --port 8086 -m /models -c 4096 -fa --chat-template chatml
    image: ghcr.io/ggerganov/llama.cpp:server
    networks:
      - anything-llm
